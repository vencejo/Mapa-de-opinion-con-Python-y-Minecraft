{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea seria hacer una busqueda de un termino en twitter , bajarse los mensajes asociados , pasarlos por un clasificador\n",
    "bayesiano y representar los resultados en Minecraft. \n",
    "\n",
    "A su vez modificando la geografia que lo anterior genera en Minecraft cambiar los parametros del clasificador Bayesiano \n",
    "para que los siguientes mensajes que lleguen sean clasificados según los nuevos parametros del clasificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Procesamiento del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    " \n",
    "with open('periodicos.json', 'r') as f:\n",
    "    tweets = json.load(f)\n",
    "    \n",
    "#pprint(tweets)\n",
    "\n",
    "tweets[\"el_pais\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE | re.UNICODE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE | re.UNICODE)\n",
    " \n",
    "def strip_non_ascii(string):\n",
    "    ''' Returns the string without non ASCII characters'''\n",
    "    stripped = (c for c in string if 0 < ord(c) < 127)\n",
    "    return ''.join(stripped)\n",
    "\n",
    "def strip_less_than_1_chars(string):\n",
    "    ''' Returns the string without non ASCII characters'''\n",
    "    stripped = (c for c in string if len(c)>3)\n",
    "    return ''.join(stripped)\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s):\n",
    "    try:\n",
    "        ascii_string = strip_non_ascii(s)\n",
    "        tokens = tokenize(ascii_string)\n",
    "        tokens = [token.lower() for token in tokens if len(token) > 3 and not token.startswith('http')]\n",
    "        return tokens\n",
    "    except TypeError:\n",
    "        return None\n",
    " \n",
    "tweet = \"RT @marcobonzanini: just an example! :D http://example.com #NLP\"\n",
    "print(preprocess(tweets[\"el_pais\"][0][1]))\n",
    "# ['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'http://example.com'.startswith('http')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_elpais_procesados = [(preprocess(tweet[1]),'positive') for tweet in tweets[\"el_pais\"]]\n",
    "print(tweets_elpais_procesados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cambiando un tweet de sentimiento\n",
    "\n",
    "tweetConNuevoSentimiento = ([u'@elpais_america', u'tras', u'ciudadana', u'corredor', u'chapultepec', u'gobierno', \\\n",
    "                             u'presentar', u'otros', u'proyectos', u'para', u'transformarlo'], 'negative')\n",
    "\n",
    "for index,tweet in enumerate(tweets_elpais_procesados):\n",
    "    if tweets_elpais_procesados[index][0] == tweetConNuevoSentimiento[0]:\n",
    "        del tweets_elpais_procesados[index]\n",
    "        tweets_elpais_procesados.append(tweetConNuevoSentimiento)\n",
    "        \n",
    "print(tweets_elpais_procesados)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "tweets_elpais_procesados = [preprocess(tweet) for (fecha,tweet) in tweets[\"el_pais\"]]\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('spanish') + punctuation + ['rt', 'via']\n",
    "\n",
    "count_all = Counter()\n",
    "terms_all = []\n",
    "\n",
    "for tweet in tweets_elpais_procesados:\n",
    "    terms_all = [term for term in tweet if term not in stop]\n",
    "    # Update the counter\n",
    "    count_all.update(terms_all)\n",
    "    \n",
    "# Print the first 5 most frequent words\n",
    "print(count_all.most_common(5)) \n",
    "print(count_all)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el codigo anterior da error quizás se tengan que descargar los recursos del nltk con las sentencias:\n",
    "\n",
    "    import nltk\n",
    "    nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count terms only once, equivalent to Document Frequency\n",
    "terms_single = set(terms_all)\n",
    "# Count hashtags only\n",
    "terms_hash = [term for term in preprocess(tweet['text']) \n",
    "              if term.startswith('#')]\n",
    "# Count terms only (no hashtags, no mentions)\n",
    "terms_only = [term for term in preprocess(tweet['text']) \n",
    "              if term not in stop and\n",
    "              not term.startswith(('#', '@'))] \n",
    "              # mind the ((double brackets))\n",
    "              # startswith() takes a tuple (not a list) if \n",
    "              # we pass a list of inputs\n",
    "\n",
    "print terms_hash\n",
    "print \"\"\n",
    "print terms_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificador Bayesiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tweets = [('I love this car', 'positive'),\n",
    "              ('This view is amazing', 'positive'),\n",
    "              ('I feel great this morning', 'positive'),\n",
    "              ('I am so excited about the concert', 'positive'),\n",
    "              ('He is my best friend', 'positive')]\n",
    "\n",
    "neg_tweets = [('I do not like this car', 'negative'),\n",
    "              ('This view is horrible', 'negative'),\n",
    "              ('I feel tired this morning', 'negative'),\n",
    "              ('I am not looking forward to the concert', 'negative'),\n",
    "              ('He is my enemy', 'negative')]\n",
    "\n",
    "test_tweets = [\n",
    "    (['feel', 'happy', 'this', 'morning'], 'positive'),\n",
    "    (['larry', 'friend'], 'positive'),\n",
    "    (['not', 'like', 'that', 'man'], 'negative'),\n",
    "    (['house', 'not', 'great'], 'negative'),\n",
    "    (['your', 'song', 'annoying'], 'negative')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for (words, sentiment) in pos_tweets + neg_tweets:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3] \n",
    "    tweets.append((words_filtered, sentiment))\n",
    "\n",
    "print (tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The list of word features need to be extracted from the tweets. \n",
    "# It is a list with every distinct words ordered by frequency of appearance.\n",
    "# We use the following function to get the list plus the two helper functions.\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "word_features = get_word_features(get_words_in_tweets(tweets))\n",
    "\n",
    "print word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To create a classifier, we need to decide what features are relevant. \n",
    "# To do that, we first need a feature extractor. \n",
    "# The one we are going to use returns a dictionary indicating what words are contained in the input passed. \n",
    "# Here, the input is the tweet. We use the word features list defined above along with the input \n",
    "# to create the dictionary\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "print(extract_features(['love', 'this', 'car']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With our feature extractor, we can apply the features to our classifier using the method apply_features. \n",
    "# We pass the feature extractor along with the tweets list defined above.\n",
    "\n",
    "training_set = nltk.classify.apply_features(extract_features, tweets)\n",
    "\n",
    "print training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now that we have our training set, we can train our classifier.\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "print classifier.show_most_informative_features(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet = 'Larry is my friend'\n",
    "print classifier.classify(extract_features(tweet.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocesadorTweets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocesadorTweets.py\n",
    "import unicodedata\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "\n",
    "def leeArchivoTweets(archivo):\n",
    "    with open(archivo, 'r') as f:\n",
    "        tweets = json.load(f)\n",
    "    return tweets\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE | re.UNICODE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE | re.UNICODE)\n",
    "\n",
    "def eliminaTildes(s):\n",
    "     return ''.join((c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn'))\n",
    "    \n",
    "def strip_non_ascii(string):\n",
    "    ''' Returns the string without non ASCII characters'''\n",
    "    stripped = (c.encode('ascii') for c in string if 0 < ord(c) < 127)\n",
    "    return ''.join(stripped)\n",
    "\n",
    "def strip_less_than_3_chars(string):\n",
    "    ''' Returns the string without non ASCII characters'''\n",
    "    stripped = (c for c in string if len(c)>3)\n",
    "    return ''.join(stripped)\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocesa(s):\n",
    "    try: \n",
    "        sinTildes = eliminaTildes(s)\n",
    "        ascii_string = strip_non_ascii(sinTildes)\n",
    "        tokens = tokenize(ascii_string)\n",
    "        tokens = [token.lower() for token in tokens if len(token) > 3 and not token.startswith('http')]\n",
    "        return tokens\n",
    "    except TypeError:\n",
    "        return None\n",
    "\n",
    "def preprocesaTweets(tweets, periodico):\n",
    "    \"\"\" El sentimiento inicial de los tweets es aleatorio\"\"\"\n",
    "    return [(preprocesa(tweet[1]),random.choice(['positive', 'negative'])) for tweet in tweets[periodico]]   \n",
    "\n",
    "def preprocesaTweet(tweet, sentimiento):\n",
    "    \"\"\" Esta funcion espera un tweet en formato ascii\"\"\"\n",
    "    tokens = tokenize(tweet)\n",
    "    tokens = [token.lower() for token in tokens if len(token) > 3 and not token.startswith('http')]\n",
    "    return (tokens,sentimiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting clasificadorBayesiano.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile clasificadorBayesiano.py\n",
    "import preprocesadorTweets as preprocesador\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "class Clasificador():\n",
    "    \n",
    "    def __init__(self, train_tweets):\n",
    "        self.word_features = self.get_word_features(self.get_words_in_tweets(train_tweets))\n",
    "        \n",
    "    # The list of word features need to be extracted from the tweets. \n",
    "    # It is a list with every distinct words ordered by frequency of appearance.\n",
    "    # We use the following function to get the list plus the two helper functions.\n",
    "\n",
    "    def get_words_in_tweets(self,tweets):\n",
    "        all_words = []\n",
    "        for (words, sentiment) in tweets:\n",
    "            all_words.extend(words)\n",
    "        return all_words\n",
    "\n",
    "    def get_word_features(self,wordlist):\n",
    "        wordlist = nltk.FreqDist(wordlist)\n",
    "        word_features = wordlist.keys()\n",
    "        return word_features\n",
    "\n",
    "\n",
    "    # To create a classifier, we need to decide what features are relevant. \n",
    "    # To do that, we first need a feature extractor. \n",
    "    # The one we are going to use returns a dictionary indicating what words are contained in the input passed. \n",
    "    # Here, the input is the tweet. We use the word features list defined above along with the input \n",
    "    # to create the dictionary\n",
    "\n",
    "    def extract_features(self,document):\n",
    "        document_words = set(document)\n",
    "        features = {}\n",
    "        for word in self.word_features:\n",
    "            features['contains(%s)' % word] = (word in document_words)\n",
    "        return features\n",
    "\n",
    "\n",
    "    def entrenaClasificador(self,train_tweets):\n",
    "        # With our feature extractor, we can apply the features to our classifier using the method apply_features. \n",
    "        # We pass the feature extractor along with the tweets list defined above.\n",
    "        training_set = nltk.classify.apply_features(self.extract_features, train_tweets)\n",
    "        # Now that we have our training set, we can train our classifier.\n",
    "        return nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "    def clasifica(self,clasificador, tweet):\n",
    "        return clasificador.classify(self.extract_features(tweet.split()))\n",
    "\n",
    "    def cambiaSentimiento(self,tweets, tweetConNuevoSentimiento, sentimiento):\n",
    "        \"\"\"Cambiando un tweet de sentimiento\n",
    "        tweetConNuevoSentimiento = 'RT @elpais_opinion: No se equivoquen, la campa\\xf1a electoral es una \n",
    "                                    extirpaci\\xf3n de los recuerdos. David Trueba explica por qu\\xe9 https://t.co/u\\u2026' \"\"\"\n",
    "\n",
    "        tweetConNuevoSentimiento = preprocesador.preprocesaTweet(tweetConNuevoSentimiento, sentimiento)\n",
    "        for index,tweet in enumerate(tweets):\n",
    "            if tweets[index][0] == tweetConNuevoSentimiento[0]:\n",
    "                del tweets[index]\n",
    "                tweets.append(tweetConNuevoSentimiento)\n",
    "        return tweets\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetConNuevoSentimiento = \"\"\"RT @elpais_opinion: No se equivoquen, la campa\\xf1a electoral es una \n",
    "                                extirpaci\\xf3n de los recuerdos. David Trueba explica por qu\\xe9 https://t.co/u\\u2026\"\"\"\n",
    "nuevosTweets = cambiaSentimiento(tweets_elpais_procesados, tweetConNuevoSentimiento, 'negative')\n",
    "print(nuevosTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "import clasificadorBayesiano\n",
    "\n",
    "train_tweets=[(['love', 'this', 'car'], 'positive'), (['this', 'view', 'amazing'], 'positive'), \n",
    "              (['feel', 'great', 'this', 'morning'], 'positive'), (['excited', 'about', 'the', 'concert'], 'positive'), \n",
    "              (['best', 'friend'], 'positive'), (['not', 'like', 'this', 'car'], 'negative'), \n",
    "              (['this', 'view', 'horrible'], 'negative'), (['feel', 'tired', 'this', 'morning'], 'negative'), \n",
    "              (['not', 'looking', 'forward', 'the', 'concert'], 'negative'), (['enemy'], 'negative')]\n",
    "\n",
    "test_tweets = [(['feel', 'happy', 'this', 'morning'], 'positive'),\n",
    "               (['larry', 'friend'], 'positive'),\n",
    "               (['not', 'like', 'that', 'man'], 'negative'),\n",
    "               (['house', 'not', 'great'], 'negative'),\n",
    "               (['your', 'song', 'annoying'], 'negative')]\n",
    "    \n",
    "# Los train_tweets iniciales son para coger las palabras sobre las que va calcular las probabilidades\n",
    "cl = clasificadorBayesiano.Clasificador(train_tweets)  \n",
    "\n",
    "# Los train_tweets siguientes son para entrenar al clasificador\n",
    "clasificador = cl.entrenaClasificador(train_tweets)\n",
    "\n",
    "#print clasificador.show_most_informative_features(32)\n",
    "\n",
    "tweet = 'Larry is my friend'\n",
    "print cl.clasifica(clasificador, tweet)\n",
    "\n",
    "#Llega mas informacion\n",
    "train_tweets.extend(test_tweets)\n",
    "\n",
    "clasificador = cl.entrenaClasificador(train_tweets)\n",
    "\n",
    "#print clasificador.show_most_informative_features(32)\n",
    "\n",
    "tweet = 'Larry is my friend'\n",
    "print cl.clasifica(clasificador, tweet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clasificacion inicial: (['menos', 'siete', 'muertos', 'ataque', 'talibanes', 'aeropuerto', 'afganistan'], 'negative')\n",
      "\n",
      "Cambio de sentimiento respecto al tweet (['menos', 'siete', 'muertos', 'ataque', 'talibanes', 'aeropuerto', 'afganistan'], 'positive')\n",
      "\n",
      "clasificacion tras cambio sentimiento: positive \n"
     ]
    }
   ],
   "source": [
    "import preprocesadorTweets as preprocesador\n",
    "import clasificadorBayesiano\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# -------------------------- Preparando y entrenando inicialmente al clasificador --------------------\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "periodicos = ['elmundoes', 'el_pais', 'abc_es','larazon_es',\n",
    "                  'eldiarioes','publico_es','20m']\n",
    "\n",
    "periodicosEntrenamiento = ['elmundoes', 'el_pais', 'abc_es','eldiarioes','20m']\n",
    "\n",
    "tweetsBase = preprocesador.leeArchivoTweets(\"periodicos_4horas.json\")\n",
    "tweetsEntrenamiento = preprocesador.leeArchivoTweets(\"periodicos.json\")\n",
    "\n",
    "tweetsBaseProcesados = []\n",
    "for periodico in periodicos:\n",
    "    tweetsBaseProcesados.extend(preprocesador.preprocesaTweets(tweetsBase,periodico))\n",
    "    \n",
    "tweetsEntrenamientoProcesados = []\n",
    "for periodico in periodicosEntrenamiento:\n",
    "    tweetsEntrenamientoProcesados.extend(preprocesador.preprocesaTweets(tweetsEntrenamiento,periodico))\n",
    "    \n",
    "#print(len(tweetsBaseProcesados))\n",
    "#print(tweetsBaseProcesados)\n",
    "#print(len(tweetsEntrenamientoProcesados))\n",
    "#print(tweetsEntrenamientoProcesados)\n",
    "\n",
    "# Los tweets iniciales son para coger las palabras sobre las que va calcular las probabilidades\n",
    "cl = clasificadorBayesiano.Clasificador(tweetsBaseProcesados)  \n",
    "\n",
    "# Los tweetsEntrenamientoProcesados siguientes son para entrenar al clasificador\n",
    "clasificador = cl.entrenaClasificador(tweetsEntrenamientoProcesados)\n",
    "\n",
    "#print clasificador.show_most_informative_features(10)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# --------Al llegar nuevos tweets los clasifico y vuelvo a entrenar al clasificador con ellos --------\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "tweetsLlegados = []\n",
    "\n",
    "nuevoTweet_texto = \"Al menos siete muertos en un ataque de los talibanes a un aeropuerto de Afganistan\"\n",
    "nuevoTweet_sentimiento = cl.clasifica(clasificador, nuevoTweet_texto)\n",
    "\n",
    "nuevoTweetProcesado = preprocesador.preprocesaTweet(nuevoTweet_texto, nuevoTweet_sentimiento)\n",
    "\n",
    "print(\"clasificacion inicial: \" + str(nuevoTweetProcesado))\n",
    "\n",
    "#Vuelvo a entrenar al clasificador con el nuevo tweet que ha entrado junto con todos los anteriores\n",
    "tweetsEntrenamientoProcesados.append(nuevoTweetProcesado)\n",
    "clasificador = cl.entrenaClasificador(tweetsEntrenamientoProcesados)\n",
    "\n",
    "#print clasificador.show_most_informative_features(10)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# -------------------------- Cambio un tweet de sentimiento  --------------------\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "tweetsEntrenamientoProcesados = cl.cambiaSentimiento(tweetsEntrenamientoProcesados, \n",
    "                                \"Al menos siete muertos en un ataque de los talibanes a un aeropuerto de Afganistan\",\n",
    "                                'positive')\n",
    "\n",
    "clasificador = cl.entrenaClasificador(tweetsEntrenamientoProcesados)\n",
    "\n",
    "#print clasificador.show_most_informative_features(10)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Cambio de sentimiento respecto al tweet \" + str(tweetsEntrenamientoProcesados[-1]))\n",
    "print(\"\")\n",
    "print (\"clasificacion tras cambio sentimiento: %s \" % cl.clasifica(clasificador, nuevoTweet_texto))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting clasificadorPrensa.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile clasificadorPrensa.py\n",
    "\n",
    "import preprocesadorTweets as preprocesador\n",
    "import clasificadorBayesiano\n",
    "\n",
    "class ClasificadorPrensa():\n",
    "    def __init__(self):\n",
    "        self.periodicos = ['elmundoes', 'el_pais', 'abc_es','larazon_es',\n",
    "                              'eldiarioes','publico_es','20m']\n",
    "\n",
    "        self.periodicosEntrenamiento = ['elmundoes', 'el_pais', 'abc_es','eldiarioes','20m']\n",
    "        self.tweetsBaseProcesados = self.extraeTweets(\"periodicos_4horas.json\", self.periodicos)\n",
    "        self.tweetsEntrenamientoProcesados  = self.extraeTweets(\"periodicos.json\", self.periodicosEntrenamiento)\n",
    "        self.clasificador = clasificadorBayesiano.Clasificador(self.tweetsBaseProcesados)  \n",
    "        self.clasificadorEntrenado = self.clasificador.entrenaClasificador(self.tweetsEntrenamientoProcesados)\n",
    "        \n",
    "    def extraeTweets(self, archivo, periodicos):     \n",
    "        tweets = preprocesador.leeArchivoTweets(archivo)\n",
    "        tweetsProcesados = []\n",
    "        for periodico in periodicos:\n",
    "            tweetsProcesados.extend(preprocesador.preprocesaTweets(tweets,periodico))\n",
    "        return tweetsProcesados\n",
    "    \n",
    "    def clasificaNuevoTweet(self, nuevoTweet_texto):\n",
    "        \"\"\" Da la clasificacion del Tweet segun el clasificador bayesiano ('positive' o 'negative') \n",
    "        y vuelve a entrenar al clasificador con esta informacion\"\"\"\n",
    "        nuevoTweet_sentimiento = self.clasificador.clasifica(self.clasificadorEntrenado, nuevoTweet_texto)\n",
    "        nuevoTweetProcesado = preprocesador.preprocesaTweet(nuevoTweet_texto, nuevoTweet_sentimiento)\n",
    "        #Vuelvo a entrenar al clasificador con el nuevo tweet que ha entrado junto con todos los anteriores\n",
    "        self.tweetsEntrenamientoProcesados.append(nuevoTweetProcesado)\n",
    "        self.clasificadorEntrenado = self.clasificador.entrenaClasificador(self.tweetsEntrenamientoProcesados)\n",
    "        return nuevoTweet_sentimiento\n",
    "        \n",
    "    def cambiaSentimiento(self, tweet_texto,nuevoSentimiento):\n",
    "        self.tweetsEntrenamientoProcesados = self.clasificador.cambiaSentimiento(self.tweetsEntrenamientoProcesados, \n",
    "                                                                                    tweet_texto, nuevoSentimiento)\n",
    "\n",
    "        self.clasificadorEntrenado = self.clasificador.entrenaClasificador(self.tweetsEntrenamientoProcesados)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "Tras el cambio de sentimiento (['menos', 'siete', 'muertos', 'ataque', 'talibanes', 'aeropuerto', 'afganistan'], 'positive')\n"
     ]
    }
   ],
   "source": [
    "c = ClasificadorPrensa()\n",
    "#print(c.tweetsEntrenamientoProcesados)\n",
    "#c.clasificadorEntrenado.show_most_informative_features(10)\n",
    "print c.clasificaNuevoTweet(\"Al menos siete muertos en un ataque de los talibanes a un aeropuerto de Afganistan\")\n",
    "#c.clasificadorEntrenado.show_most_informative_features(10)\n",
    "c.cambiaSentimiento(\"Al menos siete muertos en un ataque de los talibanes a un aeropuerto de Afganistan\",'positive')\n",
    "print(\"Tras el cambio de sentimiento \" + str(c.tweetsEntrenamientoProcesados[-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
